# services/sequence_service.py

from sqlalchemy.orm import Session
from llm.invoke import get_llm_response
from prompts.sequence_diagram_templates import build_prompt_from_summary
from schemas.sequence_request import SequenceDiagramRequest
from schemas.sequence_response import SequenceDiagramResponse
from services.api_spec_summary_service import summarize_category_api_specs
from config.logger import logger
from services.extractor import extract_plantuml_block
from utils.plantuml import plantuml_encode, validate_plantuml_code
from utils.token_utils import count_tokens
from utils.constants import get_token_constants  # âœ… ì¶”ê°€

def generate_sequence_diagram_response(request: SequenceDiagramRequest, db: Session) -> SequenceDiagramResponse:
    try:
        # 1. API ìš”ì•½ í˜¸ì¶œ
        summary_text = summarize_category_api_specs(request, db)
        if not summary_text.strip():
            raise ValueError("ìš”ì•½ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… ìš”ì•½ ê²°ê³¼")
        logger.debug(summary_text)

        # 2. ì‹œí€€ìŠ¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_prompt_from_summary(summary_text)
        prompt_token_len = count_tokens(prompt, request.llm_type)

        # 3. ëª¨ë¸ë³„ í† í° ì •ì±… ê°€ì ¸ì˜¤ê¸°
        token_config = get_token_constants(request.llm_type)
        max_tokens = (
            token_config["MAX_CONTEXT_TOKENS"]
            - prompt_token_len
            - token_config["SAFETY_MARGIN_TOKENS"]
        )

        # âœ… ì•ˆì •ì„±ì„ ìœ„í•œ ì¶”ê°€ ë§ˆì§„
        max_tokens = int(max_tokens * 0.95)

        if max_tokens < token_config["MIN_COMPLETION_TOKENS"]:
            raise ValueError("âŒ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"ğŸ”¢ ì‹œí€€ìŠ¤ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {prompt_token_len}, max_tokens: {max_tokens}")

        # 4. LLM í˜¸ì¶œ
        llm_output = get_llm_response(prompt, request.llm_type, mode="sequence", max_tokens=max_tokens)
        logger.info("âœ… LLM ì‹œí€€ìŠ¤ ìƒì„± ì‘ë‹µ")
        logger.debug(llm_output)

        # 5. PlantUML ì½”ë“œ ì¶”ì¶œ ë° ê²€ì¦
        plantuml_code = extract_plantuml_block(llm_output)
        if not validate_plantuml_code(plantuml_code):
            logger.warning("âš ï¸ ìƒì„±ëœ ì‹œí€€ìŠ¤ ì½”ë“œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
            return SequenceDiagramResponse(plantuml_code="", diagram_url=None)

        encoded = plantuml_encode(plantuml_code)
        diagram_url = f"https://www.plantuml.com/plantuml/png/{encoded}"

        logger.info("âœ… ë‹¤ì´ì–´ê·¸ë¨ ì¸ì½”ë”© ì™„ë£Œ")
        return SequenceDiagramResponse(plantuml_code=plantuml_code, diagram_url=diagram_url)

    except Exception as e:
        logger.error(f"[ERROR] ì‹œí€€ìŠ¤ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return SequenceDiagramResponse(plantuml_code="", diagram_url=None)
