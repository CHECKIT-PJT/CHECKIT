# services/api_spec_summary_service.py

from config.logger import logger
from prompts.sequence_diagram_templates import build_api_summary_prompt
from llm.invoke import get_llm_response
from sqlalchemy.orm import Session
from models.api_spec import ApiSpec
from schemas.sequence_request import SequenceDiagramRequest
from utils.token_utils import count_tokens
from utils.constants import get_token_constants


def summarize_category_api_specs(request: SequenceDiagramRequest, db: Session) -> str:
    """
    ì¹´í…Œê³ ë¦¬ë³„ API ëª…ì„¸ë¥¼ DBì—ì„œ ì¡°íšŒ í›„,
    í† í° ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§í•˜ê³  ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    token_config = get_token_constants(request.llm_type)

    # âœ… ì—¬ìœ  ìˆê²Œ ì¡°íšŒ (ë„‰ë„‰íˆ)
    candidate_specs = db.query(
        ApiSpec.method,
        ApiSpec.endpoint,
        ApiSpec.description
    ).filter(
        ApiSpec.project_id == request.project_id,
        ApiSpec.category == request.category,
        ApiSpec.is_deleted == 0
    ).order_by(ApiSpec.created_at.desc()).limit(30).all()

    # âœ… í…œí”Œë¦¿ ê¸°ì¤€ í† í° ê³„ì‚°
    base_prompt = build_api_summary_prompt([])
    base_prompt_tokens = count_tokens(base_prompt, request.llm_type)

    max_data_tokens = (
        token_config["MAX_CONTEXT_TOKENS"]
        - token_config["SAFETY_MARGIN_TOKENS"]
        - token_config["MIN_COMPLETION_TOKENS"]
        - base_prompt_tokens
    )

    logger.info(f"ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ í† í° ì—¬ìœ : {max_data_tokens} (í”„ë¡¬í”„íŠ¸ ì œì™¸)")

    # âœ… í† í° ê¸°ì¤€ìœ¼ë¡œ ëˆ„ì í•˜ë©´ì„œ ìë¥´ê¸°
    filtered = []
    token_sum = 0
    for spec in candidate_specs:
        line = f"[{spec.method}] {spec.endpoint} - {spec.description or ''}"
        line_tokens = count_tokens(line, request.llm_type)
        if token_sum + line_tokens > max_data_tokens:
            break
        filtered.append(spec)
        token_sum += line_tokens

    if not filtered:
        raise ValueError("âŒ ìš”ì•½ ê°€ëŠ¥í•œ API ëª…ì„¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚´ìš©ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.")

    prompt = build_api_summary_prompt(filtered)
    prompt_token_len = count_tokens(prompt, request.llm_type)
    max_tokens = token_config["MAX_CONTEXT_TOKENS"] - prompt_token_len - token_config["SAFETY_MARGIN_TOKENS"]

    if max_tokens < token_config["MIN_COMPLETION_TOKENS"]:
        raise ValueError("âŒ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    logger.info(f"ğŸ§¾ ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_token_len} tokens, max_tokens: {max_tokens}")
    return get_llm_response(prompt, request.llm_type, mode="summary", max_tokens=max_tokens)
