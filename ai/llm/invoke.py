# llm/invoke.py

import time
from llm.factory import get_llm
from config.logger import logger
from utils.token_utils import count_tokens  # ğŸ”¹ ì¶”ê°€

def get_llm_response(prompt: str, model_type: str, mode: str = "default", max_tokens: int = None, stream: bool = False) -> str:
    logger.info(f"LLM í˜¸ì¶œ ì‹œì‘ [ëª¨ë¸: {model_type}, ëª¨ë“œ: {mode}, max_tokens: {max_tokens}]")
    start = time.time()
    try:
        prompt_token_len = count_tokens(prompt)  # ğŸ”¹ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì¸¡ì •
        logger.info(f"ğŸ”¢ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {prompt_token_len}, ì˜ˆìƒ ì‘ë‹µ í† í°: {max_tokens}, ì´ ìš”ì²­ í† í°: {prompt_token_len + (max_tokens or 0)}")

        llm = get_llm(model_type)
        response = llm.generate(prompt, mode=mode, max_tokens=max_tokens, stream=stream)

        logger.info(f"LLM í”„ë¡¬í”„íŠ¸:\n{prompt}")
        logger.info(f"LLM ì‘ë‹µ ë³¸ë¬¸:\n{response}")
        logger.info(f"LLM ì‘ë‹µ ì™„ë£Œ â±{time.time() - start:.2f}s")
        return response
    except Exception as e:
        logger.exception("LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise
